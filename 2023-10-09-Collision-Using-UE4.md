---
title: "YOLOv5ì™€ Unreal Engine4ë¥¼ í™œìš©í•œ Sensorless AMR ì¶©ëŒë°©ì§€"
date: 2023-10-09
categories: ROS Robot operation system YOLOv5 camera unrealengine4 sensorless tcp amr turtlebot collision detect
---

# MySQL Server

## Database configuration

![sql_one_1](https://github.com/JeongMin-D/GraduationProject/assets/38097923/86efe777-adf1-41bd-a535-553de7f6f38b)

- ê° x, y, thetaì˜ ê°’ì„ DOUBLEë¡œ ì„¤ì •í•˜ì—¬ ë°ì´í„°ë¥¼ ê³µìœ í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •

![sql_one](https://github.com/JeongMin-D/GraduationProject/assets/38097923/c644c501-445d-40e9-ac40-ceb51e545bec)

- one í…Œì´ë¸”: Turtlebot3ì˜ Odometryë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì—…ë¡œë“œí•˜ì—¬ ì‹¤ì‹œê°„ í„°í‹€ë´‡ ìœ„ì¹˜ ê³µìœ 
- í†µì‹  ì†ë„ë¥¼ ìœ„í•´ ì§€ì†ì ìœ¼ë¡œ insert í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ uploadí•˜ëŠ” ë°©ì‹ ì‚¬ìš©

![sql_two_1](https://github.com/JeongMin-D/GraduationProject/assets/38097923/736afb53-d019-46e4-a6a5-d151148f11e3)

- YOLOv5ì—ì„œ ê°ì²´ë¥¼ ì°¾ì€ ì •ë³´ ì¤‘ IDëŠ” INT í˜•, Labelì€ Varcharí˜•, ConfidenceëŠ” Floatí˜•, x y w hëŠ” Doubleí˜•, timestampëŠ” Time í˜•ìœ¼ë¡œ ì§€ì •

![sql_two](https://github.com/JeongMin-D/GraduationProject/assets/38097923/2129bc16-8bff-48ee-a47a-0e0c5daec9a0)

- two í…Œì´ë¸”: YOLOv5ì—ì„œ ê°ì²´ë¥¼ ì°¾ì€ ì •ë³´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì—…ë¡œë“œí•˜ì—¬ ê°ì²´ ì •ë³´ì™€ ìœ„ì¹˜ ê³µìœ 
- í†µì‹  ì†ë„ë¥¼ ìœ„í•´ ì‹¤ì‹œê°„ìœ¼ë¡œ ê°ì²´ë¥¼ íƒì§€í•˜ë©´ insertê°€ ì•„ë‹Œ updateë¡œ ì—…ë¡œë“œí•˜ê³  5ì´ˆê°„ ìƒˆë¡œìš´ ì—…ë°ì´íŠ¸ê°€ ë˜ì§€ ì•Šìœ¼ë©´ ì‚­ì œ

![sql_three_1](https://github.com/JeongMin-D/GraduationProject/assets/38097923/045112e1-6167-4eba-afed-b7829fec1a6b)

- stop ë°ì´í„°ë¥¼ INTí˜•ìœ¼ë¡œ ì„¤ì •

![sql_three](https://github.com/JeongMin-D/GraduationProject/assets/38097923/ba1c44ff-d1b5-492c-b95a-fa2d095fbab8)

- three í…Œì´ë¸”: ì¶©ëŒ ì‹ í˜¸ì— ë”°ë¼ stop ê°’ì´ True or Falseì™€ ê°™ì´ 1ê³¼ 0ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì–´ ì‹ í˜¸

# YOLOv5

## Custom data training
[Roboflow](https://universe.roboflow.com/)

- Roboflowì˜ ë°•ìŠ¤ ë°ì´í„°ë¥¼ ë‹¤ìš´ ë°›ì•„ì„œ ë°•ìŠ¤ ë°ì´í„° í•™ìŠµ ì§„í–‰

![custom_data](https://github.com/JeongMin-D/GraduationProject/assets/38097923/08c949a0-d5b7-4aaa-9b47-64aaba59bc6f)

- ê°™ì€ ê³µê°„ ë‚´ì— ëª¨ë“  íŒŒì¼ì„ ìœ„ì¹˜ì‹œì¼œì•¼ í•™ìŠµ ì§„í–‰ ì›í™œ

```commandline
python train.py --img 416 --batch 16 --epochs 50 --data ./package/data.yaml --cfg ./models/yolov5s.yaml --weights yolov5s.pt --name box_yolov5s_results
```

- ê° ê°’ë“¤ì€ ì‚¬ìš©ì ì§€ì •ì— ë”°ë¼ ì„¤ì •í•˜ê³  --data ê°’ì€ í•´ë‹¹ ê²½ë¡œ ë‚´ì˜ yaml íŒŒì¼ë¡œ ì„¤ì •

## Training file validation

```commandline
python val.py --data ë°ì´í„° í•™ìŠµí•  ë•Œì˜ yaml íŒŒì¼.yaml --weights í•™ìŠµ í›„ ìƒì„±ëœ best.pt íŒŒì¼ best.pt --img 640 --batch-size 32
```

- í•™ìŠµí•  ë•Œ ì…ë ¥í•˜ì˜€ë˜ yaml íŒŒì¼ê³¼ í•™ìŠµ í›„ ìƒì„±ëœ best.pt íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ íŠ¸ë ˆì´ë‹ ë°ì´í„° í™•ì¸

![PR_curve](https://github.com/JeongMin-D/GraduationProject/assets/38097923/be0a6f19-41cc-4fb9-8389-0fbd39c0bb45)

![val_batch2_pred](https://github.com/JeongMin-D/GraduationProject/assets/38097923/7192db4f-fe56-49f9-b2bb-eb4be5b31d4b)

- validation ì§„í–‰ í›„ ë‹¤ìŒê³¼ ê°™ì€ ê·¸ë˜í”„ ë° íŒŒì¼ì´ ìƒì„±ë˜ê³ , ì´ë•Œ IoUê°€ 0.5ì¼ ë•Œ, mAPê°€ 98.3%ê°€ ë‚˜ì˜¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

## Object detection, Location estimation and Upload data to MySQL

```python
# YOLOv5 ğŸš€ by Ultralytics, AGPL-3.0 license
"""
Run YOLOv5 detection inference on images, videos, directories, globs, YouTube, webcam, streams, etc.

Usage - sources:
    $ python detect.py --weights yolov5s.pt --source 0                               # webcam
                                                     img.jpg                         # image
                                                     vid.mp4                         # video
                                                     screen                          # screenshot
                                                     path/                           # directory
                                                     list.txt                        # list of images
                                                     list.streams                    # list of streams
                                                     'path/*.jpg'                    # glob
                                                     'https://youtu.be/LNwODJXcvt4'  # YouTube
                                                     'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream

Usage - formats:
    $ python detect.py --weights yolov5s.pt                 # PyTorch
                                 yolov5s.torchscript        # TorchScript
                                 yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                                 yolov5s_openvino_model     # OpenVINO
                                 yolov5s.engine             # TensorRT
                                 yolov5s.mlmodel            # CoreML (macOS-only)
                                 yolov5s_saved_model        # TensorFlow SavedModel
                                 yolov5s.pb                 # TensorFlow GraphDef
                                 yolov5s.tflite             # TensorFlow Lite
                                 yolov5s_edgetpu.tflite     # TensorFlow Edge TPU
                                 yolov5s_paddle_model       # PaddlePaddle
"""

import argparse
import csv
import os
import platform
import sys
from pathlib import Path

import torch

import pymysql

FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]  # YOLOv5 root directory
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # add ROOT to PATH
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative

from ultralytics.utils.plotting import Annotator, colors, save_one_box

from models.common import DetectMultiBackend
from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams
from utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,
                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh)
from utils.torch_utils import select_device, smart_inference_mode


@smart_inference_mode()
def run(
        weights=ROOT / 'yolov5s.pt',  # model path or triton URL
        source=ROOT / 'data/images',  # file/dir/URL/glob/screen/0(webcam)
        data=ROOT / 'data/coco128.yaml',  # dataset.yaml path
        imgsz=(640, 640),  # inference size (height, width)
        conf_thres=0.25,  # confidence threshold
        iou_thres=0.45,  # NMS IOU threshold
        max_det=1000,  # maximum detections per image
        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu
        view_img=False,  # show results
        save_txt=False,  # save results to *.txt
        save_csv=False,  # save results in CSV format
        save_conf=False,  # save confidences in --save-txt labels
        save_crop=False,  # save cropped prediction boxes
        nosave=False,  # do not save images/videos
        classes=None,  # filter by class: --class 0, or --class 0 2 3
        agnostic_nms=False,  # class-agnostic NMS
        augment=False,  # augmented inference
        visualize=False,  # visualize features
        update=False,  # update all models
        project=ROOT / 'runs/detect',  # save results to project/name
        name='exp',  # save results to project/name
        exist_ok=False,  # existing project/name ok, do not increment
        line_thickness=3,  # bounding box thickness (pixels)
        hide_labels=False,  # hide labels
        hide_conf=False,  # hide confidences
        half=False,  # use FP16 half-precision inference
        dnn=False,  # use OpenCV DNN for ONNX inference
        vid_stride=1,  # video frame-rate stride
):
    source = str(source)
    save_img = not nosave and not source.endswith('.txt')  # save inference images
    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)
    is_url = source.lower().startswith(('rtsp://', 'rtmp://', 'http://', 'https://'))
    webcam = source.isnumeric() or source.endswith('.streams') or (is_url and not is_file)
    screenshot = source.lower().startswith('screen')
    if is_url and is_file:
        source = check_file(source)  # download

    # Directories
    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run
    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir

    # Load model
    device = select_device(device)
    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)
    stride, names, pt = model.stride, model.names, model.pt
    imgsz = check_img_size(imgsz, s=stride)  # check image size

    # Dataloader
    bs = 1  # batch_size
    if webcam:
        view_img = check_imshow(warn=True)
        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)
        bs = len(dataset)
    elif screenshot:
        dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)
    else:
        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)
    vid_path, vid_writer = [None] * bs, [None] * bs

    # Run inference
    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup
    seen, windows, dt = 0, [], (Profile(), Profile(), Profile())
    frame_count = 0  # í”„ë ˆì„ ì¹´ìš´íŠ¸ ë³€ìˆ˜ ì¶”ê°€
    for path, im, im0s, vid_cap, s in dataset:
        with dt[0]:
            im = torch.from_numpy(im).to(model.device)
            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32
            im /= 255  # 0 - 255 to 0.0 - 1.0
            if len(im.shape) == 3:
                im = im[None]  # expand for batch dim

        # Inference
        with dt[1]:
            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False
            pred = model(im, augment=augment, visualize=visualize)

        # NMS
        with dt[2]:
            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)

        # Second-stage classifier (optional)
        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)

        # Define the path for the CSV file
        csv_path = save_dir / 'predictions.csv'

        # Create or append to the CSV file
        def write_to_csv(image_name, prediction, confidence):
            data = {'Image Name': image_name, 'Prediction': prediction, 'Confidence': confidence}
            with open(csv_path, mode='a', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=data.keys())
                if not csv_path.is_file():
                    writer.writeheader()
                writer.writerow(data)

        # ì»¤ë„¥ì…˜ ì´ˆê¸°í™”
        def initialize_mysql_connection():
            conn = pymysql.connect(
                host='192.168.1.155',
                user='turtlebot',
                password='0000',
                db='test',
                charset='utf8mb4',
                cursorclass=pymysql.cursors.DictCursor
            )
            return conn

        # SQL ë‚´ ê°ì²´ ì¡´ì¬ ì‹œ ì—…ë°ì´íŠ¸, ì—†ì„ ì‹œ ì¶”ê°€
        def insert_data_into_mysql(object_id, label, confidence, x, y, w, h):
            conn = initialize_mysql_connection()
            try:
                with conn.cursor() as cursor:
                    sql_check = "SELECT * FROM two WHERE object_id=%s AND label=%s"
                    cursor.execute(sql_check, (object_id, label))
                    existing_record = cursor.fetchone()

                    if existing_record:
                        sql_update = "UPDATE two SET confidence=%s, x_cm=%s, y_cm=%s, w=%s, h=%s, timestamp=NOW() WHERE object_id=%s AND label=%s"
                        cursor.execute(sql_update, (confidence, x, y, w, h, object_id, label))
                    else:
                        sql_insert = "INSERT INTO two (object_id, label, confidence, x_cm, y_cm, w, h, timestamp) VALUES (%s, %s, %s, %s, %s, %s, %s, NOW())"
                        cursor.execute(sql_insert, (object_id, label, confidence, x, y, w, h))

                conn.commit()
            finally:
                conn.close()

        # timestampì˜ ì‹œê°„ì´ 5ì´ˆê°€ íë¥´ë©´ ë°ì´í„° ì‚­ì œ
        def delete_inactive_objects():
            conn = initialize_mysql_connection()
            try:
                with conn.cursor() as cursor:
                    cursor.execute("SELECT NOW() - INTERVAL 5 SECOND as threshold_time")
                    threshold_time = cursor.fetchone()['threshold_time']

                    sql_delete_inactive = "DELETE FROM two WHERE timestamp < %s"
                    cursor.execute(sql_delete_inactive, (threshold_time,))

                conn.commit()
            finally:
                conn.close()

        # Initialize dictionary to store object IDs
        object_ids = {}

        # Process predictions
        for i, det in enumerate(pred):  # per image
            seen += 1
            frame_count += 1
            if frame_count % 5 == 0:
                delete_inactive_objects()
            if webcam:  # batch_size >= 1
                p, im0, frame = path[i], im0s[i].copy(), dataset.count
                s += f'{i}: '
            else:
                p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)

            p = Path(p)  # to Path
            save_path = str(save_dir / p.name)  # im.jpg
            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # im.txt
            s += '%gx%g ' % im.shape[2:]  # print string
            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh
            imc = im0.copy() if save_crop else im0  # for save_crop
            annotator = Annotator(im0, line_width=line_thickness, example=str(names))
            if len(det):
                # Rescale boxes from img_size to im0 size
                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()

                # Print results
                for c in det[:, 5].unique():
                    n = (det[:, 5] == c).sum()  # detections per class
                    s += f"{n} {names[int(c)]}{'s' * (n > 1)}, "  # add to string

                # Write results
                for *xyxy, conf, cls in reversed(det):
                    c = int(cls)  # integer class
                    label = names[c] if hide_conf else f'{names[c]}'
                    confidence = float(conf)
                    confidence_str = f'{confidence:.2f}'

                    # Get object ID (or assign a new one)
                    if c not in object_ids:
                        object_ids[c] = 1
                    else:
                        object_ids[c] += 1
                    object_id = object_ids[c]

                    # Print coordinates
                    x, y, w, h = map(int, xyxy)  # Convert to integers
                    x_cm = x * 0.0264583333
                    y_cm = y * 0.0264583333
                    w_cm = w * 0.0264583333
                    h_cm = h * 0.0264583333
                    print(
                        f'Object ID: {object_id}, Label: {label}, Confidence: {confidence_str}, Coordinates: x={x_cm}, y={y_cm}, w={w_cm}, h={h_cm}')

                    insert_data_into_mysql(object_id, label, confidence, x_cm, y_cm, w_cm, h_cm)

                    if save_csv:
                        write_to_csv(p.name, label, confidence_str)

                    if save_txt:  # Write to file
                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh
                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format
                        with open(f'{txt_path}.txt', 'a') as f:
                            f.write(('%g ' * len(line)).rstrip() % line + '\n')

                    if save_img or save_crop or view_img:  # Add bbox to image
                        c = int(cls)  # integer class
                        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]}{object_id} {conf:.2f}')
                        annotator.box_label(xyxy, label, color=colors(c, True))
                    if save_crop:
                        save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)

            # Stream results
            im0 = annotator.result()
            if view_img:
                if platform.system() == 'Linux' and p not in windows:
                    windows.append(p)
                    cv2.namedWindow(str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)  # allow window resize (Linux)
                    cv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])
                cv2.imshow(str(p), im0)
                cv2.waitKey(1)  # 1 millisecond

            # Save results (image with detections)
            if save_img:
                if dataset.mode == 'image':
                    cv2.imwrite(save_path, im0)
                else:  # 'video' or 'stream'
                    if vid_path[i] != save_path:  # new video
                        vid_path[i] = save_path
                        if isinstance(vid_writer[i], cv2.VideoWriter):
                            vid_writer[i].release()  # release previous video writer
                        if vid_cap:  # video
                            fps = vid_cap.get(cv2.CAP_PROP_FPS)
                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                        else:  # stream
                            fps, w, h = 30, im0.shape[1], im0.shape[0]
                        save_path = str(Path(save_path).with_suffix('.mp4'))  # force *.mp4 suffix on results videos
                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))
                    vid_writer[i].write(im0)

        # Print time (inference-only)
        #LOGGER.info(f"{s}{'' if len(det) else '(no detections), '}{dt[1].dt * 1E3:.1f}ms")

    # Print results
    t = tuple(x.t / seen * 1E3 for x in dt)  # speeds per image
    LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}' % t)
    if save_txt or save_img:
        s = f"\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}" if save_txt else ''
        LOGGER.info(f"Results saved to {colorstr('bold', save_dir)}{s}")
    if update:
        strip_optimizer(weights[0])  # update model (to fix SourceChangeWarning)


def parse_opt():
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path or triton URL')
    parser.add_argument('--source', type=str, default=ROOT / 'data/images', help='file/dir/URL/glob/screen/0(webcam)')
    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='(optional) dataset.yaml path')
    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[1000], help='inference size h,w')
    parser.add_argument('--conf-thres', type=float, default=0.4, help='confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.4, help='NMS IoU threshold')
    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--view-img', action='store_true', help='show results')
    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')
    parser.add_argument('--save-csv', action='store_true', help='save results in CSV format')
    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')
    parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')
    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--visualize', action='store_true', help='visualize features')
    parser.add_argument('--update', action='store_true', help='update all models')
    parser.add_argument('--project', default=ROOT / 'runs/detect', help='save results to project/name')
    parser.add_argument('--name', default='exp', help='save results to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')
    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')
    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')
    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')
    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')
    parser.add_argument('--vid-stride', type=int, default=1, help='video frame-rate stride')
    opt = parser.parse_args()
    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand
    print_args(vars(opt))
    return opt


def main(opt):
    check_requirements(ROOT / 'requirements.txt', exclude=('tensorboard', 'thop'))
    run(**vars(opt))


if __name__ == '__main__':
    opt = parse_opt()
    main(opt)
```

```commandline
python test4.py --weights best.pt --source 0 --device 0 --conf 0.6 --line-thickness 2
```
- ì»¤ìŠ¤í…€ ë°ì´í„°ë¥¼ í•™ìŠµí•œ best.ptì™€ ì›¹ìº , GPUë¥¼ ì‚¬ìš©í•˜ê³  confidence 0.6ì´ìƒë§Œ ê²€ì¶œ

### ë™ì¼ ê°ì²´ì— ëŒ€í•œ ID ë¶€ì—¬

```python
# Initialize dictionary to store object IDs
        object_ids = {}

# Get object ID (or assign a new one)
                    if c not in object_ids:
                        object_ids[c] = 1
                    else:
                        object_ids[c] += 1
                    object_id = object_ids[c]
```

- íƒì§€ ê°ì²´ê°€ object_ids(ë”•ì…”ë„ˆë¦¬) ì•ˆì— ì—†ì„ ë•Œ 1ë²ˆ IDë¥¼ ë¶€ì—¬
- ì´ë¯¸ object_ids(ë”•ì…”ë„ˆë¦¬) ì•ˆì— ì¡´ì¬í•  ê²½ìš° IDì— 1ì”© ë”í•˜ë©° ìˆœì°¨ì ìœ¼ë¡œ ID ë¶€ì—¬

### í”½ì…€ì„ ì‚¬ìš©í•œ ìœ„ì¹˜ ì¶”ì •

```python
# Print coordinates
                    x, y, w, h = map(int, xyxy)  # Convert to integers
                    x_cm = x * 0.0264583333
                    y_cm = y * 0.0264583333
                    w_cm = w * 0.0264583333
                    h_cm = h * 0.0264583333
```

- DPI: í”½ì…€ ë°€ë„ í˜¹ì€ ì¸ì¹˜ë‹¹ ë„íŠ¸ ìˆ˜
- 96 DPI = 96PX/1INCH
- 1 INCH = 2.54CM
- 96PX = 2.54CM
- 1PX = 2.54CM / 96 = 0.026458333CM
- ì™¼ìª½ ìƒë‹¨ì„ ê¸°ì¤€ìœ¼ë¡œ xëŠ” ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½, yëŠ” ìœ„ì—ì„œ ì•„ë˜ë¡œ ì¦ê°€ì‹œì¼œ í”½ì…€ ê°’ì„ ìƒì„±í•˜ê³  cmë¡œ ë³€í™˜
- wëŠ” ê°ì§€ëœ ê°ì²´ì˜ ë„ˆë¹„ë¥¼ í”½ì…€ë¡œ ë‚˜íƒ€ë‚´ê³  hëŠ” ë†’ì´ë¥¼ ë‚˜íƒ€ëƒ„

### MySQLì— íƒì§€ ê°ì²´ ì—…ë¡œë“œ ë° ì‚­ì œ

```python
        def initialize_mysql_connection():
            conn = pymysql.connect(
                host='192.168.1.155',
                user='turtlebot',
                password='0000',
                db='test',
                charset='utf8mb4',
                cursorclass=pymysql.cursors.DictCursor
            )
            return conn

        # SQL ë‚´ ê°ì²´ ì¡´ì¬ ì‹œ ì—…ë°ì´íŠ¸, ì—†ì„ ì‹œ ì¶”ê°€
        def insert_data_into_mysql(object_id, label, confidence, x, y, w, h):
            conn = initialize_mysql_connection()
            try:
                with conn.cursor() as cursor:
                    sql_check = "SELECT * FROM two WHERE object_id=%s AND label=%s"
                    cursor.execute(sql_check, (object_id, label))
                    existing_record = cursor.fetchone()

                    if existing_record:
                        sql_update = "UPDATE two SET confidence=%s, x=%s, y=%s, w=%s, h=%s, timestamp=NOW() WHERE object_id=%s AND label=%s"
                        cursor.execute(sql_update, (confidence, x, y, w, h, object_id, label))
                    else:
                        sql_insert = "INSERT INTO two (object_id, label, confidence, x, y, w, h, timestamp) VALUES (%s, %s, %s, %s, %s, %s, %s, NOW())"
                        cursor.execute(sql_insert, (object_id, label, confidence, x, y, w, h))

                conn.commit()
            finally:
                conn.close()

        # timestampì˜ ì‹œê°„ì´ 5ì´ˆê°€ íë¥´ë©´ ë°ì´í„° ì‚­ì œ
        def delete_inactive_objects():
            conn = initialize_mysql_connection()
            try:
                with conn.cursor() as cursor:
                    cursor.execute("SELECT NOW() - INTERVAL 5 SECOND as threshold_time")
                    threshold_time = cursor.fetchone()['threshold_time']

                    sql_delete_inactive = "DELETE FROM two WHERE timestamp < %s"
                    cursor.execute(sql_delete_inactive, (threshold_time,))

                conn.commit()
            finally:
                conn.close()
            frame_count += 1
            if frame_count % 5 == 0:
                delete_inactive_objects()
                insert_data_into_mysql(object_id, label, confidence, x_cm, y_cm, w_cm, h_cm)
```

- initialize_mysql_connection(): pymysqlì„ ì‚¬ìš©í•˜ì—¬ host, user, password, db ë“± í†µì‹ ì— í•„ìš”í•œ ê¸°ë³¸ ì •ë³´ ì„¤ì •
- insert_data_into_mysql(object_id, label, confidence, x, y, w, h): ê°ì²´ íƒì§€ ì¤‘ ìƒì„±í•œ ID, ê°ì²´ ë¼ë²¨, conf, ì¢Œí‘œ ë° ë„ˆë¹„, ë†’ì´ ë“±ì„ Mysql ë°ì´í„°ë² ì´ìŠ¤ì— ì—…ë¡œë“œ
- delete_inactive_objects(): ì—…ë¡œë“œ ëœ ê°ì²´ ì¤‘ 5ì´ˆê°„ ì—…ë°ì´íŠ¸ ë˜ì§€ ì•Šì€ ê°ì²´ëŠ” ì‚­ì œ

# AMR

## Custom DWA Algorithm

```python
#!/usr/bin/env python

import rospy
from geometry_msgs.msg import Twist
from nav_msgs.msg import Odometry
import math
import random
from tf.transformations import euler_from_quaternion
import pymysql

class TurtleBot:
    def __init__(self):
        rospy.init_node('turtlebot_controller', anonymous=False)
        rospy.on_shutdown(self.shutdown)

        self.cmd_vel = rospy.Publisher('cmd_vel', Twist, queue_size=10)
        self.odom_sub = rospy.Subscriber('odom', Odometry, self.odom_callback)

        self.rate = rospy.Rate(10)  # 10 Hz
        self.target_coordinates = []  # List to store target coordinates
        self.current_target_index = 0  # Index of the current target coordinate

        self.max_linear_velocity = 0.2
        self.max_angular_velocity = 1.0
        self.max_linear_acceleration = 0.1
        self.max_angular_acceleration = 1.0

        self.scan_ranges = None
        self.scan_angle_min = None
        self.scan_angle_increment = None

        self.current_pose = None  # Store current pose

        # Connect to the database
        self.conn = pymysql.connect(host='192.168.1.155', user='turtlebot', password='0000', database='test')
        self.cursor = self.conn.cursor()

    def insert_coordinates(self, x, y, theta):
        try:
            # Connect to the database
            conn = pymysql.connect(host='192.168.1.155', user='turtlebot', password='0000', database='test')
            cursor = conn.cursor()

            query = "UPDATE one SET x = %s, y = %s, theta = %s WHERE idone = 1"
            self.cursor.execute(query, (x, y, theta))
            self.conn.commit()

        except pymysql.Error as e:
            print(f"Error in update_position: {e}")

    def check_stop(self):
        try:
            # Retrieve the collision value from the three table
            query = "SELECT stop FROM three order by idthree desc limit 1"
            self.cursor.execute(query)
            result = self.cursor.fetchone()

            if result is None:
                return 0
            else:
                return result[0]

        except pymysql.Error as e:
            print(f"Error in check_collision: {e}")
            return 0

    def odom_callback(self, data):
        self.current_pose = data.pose.pose  # Update current pose

        # Check if there are any target coordinates
        if len(self.target_coordinates) == 0:
            return

        # Calculate the Euclidean distance between the current position and the current target position
        current_x = self.current_pose.position.x
        current_y = self.current_pose.position.y

        # Find current yaw
        # current_orientation = (
        #     self.current_pose.orientation.x,
        #     self.current_pose.orientation.y,
        #     self.current_pose.orientation.z,
        #     self.current_pose.orientation.w
        # )
        # _, _, current_yaw = euler_from_quaternion(current_orientation)

        distance_to_target = math.sqrt((self.target_coordinates[self.current_target_index][0] - current_x) ** 2 +
                                       (self.target_coordinates[self.current_target_index][1] - current_y) ** 2)

        # Check if the TurtleBot has reached the current target position
        if distance_to_target <= 0.1:
            rospy.loginfo("Target reached: {}".format(self.target_coordinates[self.current_target_index]))
            self.current_target_index += 1  # Move to the next target coordinate

            if self.current_target_index >= len(self.target_coordinates):
                self.cmd_vel.publish(Twist())  # Stop the TurtleBot
                rospy.loginfo("All target positions reached!")
                rospy.signal_shutdown("All target positions reached!")  # Shutdown the node

                # Insert current coordinates into the database
                # self.insert_coordinates(current_x, current_y, current_yaw)

    def move_to_target(self, target_coordinates):
        self.target_coordinates = target_coordinates

        for i in range(len(self.target_coordinates)):
            target_x, target_y = self.target_coordinates[i]
            rospy.loginfo("Moving to target: ({}, {})".format(target_x, target_y))

            while not rospy.is_shutdown():
                # Get the current target coordinates
                target_x, target_y = self.target_coordinates[i]

                # Calculate the distance to the current target
                current_x = self.current_pose.position.x
                current_y = self.current_pose.position.y
                distance_to_target = math.sqrt((target_x - current_x) ** 2 + (target_y - current_y) ** 2)

                # Calculate the angle to the target
                target_angle = math.atan2(target_y - current_y, target_x - current_x)

                # Calculate the angular velocity to align with the target angle
                current_orientation = (
                    self.current_pose.orientation.x,
                    self.current_pose.orientation.y,
                    self.current_pose.orientation.z,
                    self.current_pose.orientation.w
                )
                _, _, current_yaw = euler_from_quaternion(current_orientation)
                angle_difference = target_angle - current_yaw

                # Insert current coordinates into the database
                self.insert_coordinates(current_x, current_y, current_yaw)

                # Normalize the angle difference to the range [-pi, pi]
                angle_difference = math.atan2(math.sin(angle_difference), math.cos(angle_difference))

                # Calculate the linear velocity based on the distance to the target
                linear_velocity = self.max_linear_velocity * distance_to_target

                # Limit the linear velocity within the maximum limits
                linear_velocity = max(0.0, min(self.max_linear_velocity, linear_velocity))

                # Calculate the angular velocity to align with the target angle
                angular_velocity = self.max_angular_velocity * angle_difference

                # Limit the angular velocity within the maximum limits
                angular_velocity = max(-self.max_angular_velocity, min(self.max_angular_velocity, angular_velocity))

                # Publish the velocities to move towards the target while aligning with the target angle
                twist_cmd = Twist()
                twist_cmd.linear.x = linear_velocity
                twist_cmd.angular.z = angular_velocity
                self.cmd_vel.publish(twist_cmd)

                # If the distance to the target is less than a threshold, stop and break the loop
                if distance_to_target <= 0.1:
                    rospy.loginfo("Target reached: ({}, {})".format(target_x, target_y))
                    twist_cmd = Twist()
                    twist_cmd.angular.z = 0.0
                    twist_cmd.linear.x = 0.0
                    self.cmd_vel.publish(twist_cmd)
                    break

                # Check for stop signal from the database
                if self.check_stop() == 1:
                    twist_cmd = Twist()
                    twist_cmd.angular.z = 0.0
                    twist_cmd.linear.x = 0.0
                    self.cmd_vel.publish(twist_cmd)
                    rospy.loginfo("Stopped by external signal.")
                    return

                self.rate.sleep()

            # Stop the TurtleBot before moving to the next target
            twist_cmd = Twist()
            twist_cmd.angular.z = 0.0
            twist_cmd.linear.x = 0.0
            self.cmd_vel.publish(twist_cmd)
            self.rate.sleep()

        rospy.loginfo("All target positions reached!")
        rospy.signal_shutdown("All target positions reached!")  # Shutdown the node

    def calculate_dwa(self):
        if self.scan_ranges is None or self.scan_angle_min is None or self.scan_angle_increment is None:
            return 0.0, 0.0

        # DWA algorithm implementation
        min_cost = float('inf')
        best_linear_velocity = 0.0
        best_angular_velocity = 0.0

        for linear_velocity in self.generate_linear_velocities():
            for angular_velocity in self.generate_angular_velocities():
                # Simulate the robot's trajectory to calculate the cost of the trajectory
                simulated_trajectory_cost = self.simulate_trajectory(linear_velocity, angular_velocity)

                # Choose the trajectory with the lowest cost
                if simulated_trajectory_cost < min_cost:
                    min_cost = simulated_trajectory_cost
                    best_linear_velocity = linear_velocity
                    best_angular_velocity = angular_velocity

        return best_linear_velocity, best_angular_velocity

    def simulate_trajectory(self, linear_velocity, angular_velocity):
        # Simulate the robot's trajectory using the provided linear and angular velocities
        # and calculate the cost of the trajectory

        # Define some simulation parameters
        num_steps = 50
        dt = 0.1
        trajectory_cost = 0.0

        # Simulate the trajectory and accumulate cost
        for step in range(num_steps):
            # Simulate the robot's motion using the given velocities
            linear_distance = linear_velocity * dt
            angular_distance = angular_velocity * dt

            # Update the robot's position and orientation
            current_x = self.current_pose.position.x
            current_y = self.current_pose.position.y
            current_orientation = (
                self.current_pose.orientation.x,
                self.current_pose.orientation.y,
                self.current_pose.orientation.z,
                self.current_pose.orientation.w
            )
            _, _, current_yaw = euler_from_quaternion(current_orientation)

            new_x = current_x + linear_distance * math.cos(current_yaw)
            new_y = current_y + linear_distance * math.sin(current_yaw)
            new_yaw = current_yaw + angular_distance

            # Calculate the cost based on the distance to the target and collision risk
            distance_to_target = math.sqrt((self.target_coordinates[self.current_target_index][0] - new_x) ** 2 +
                                           (self.target_coordinates[self.current_target_index][1] - new_y) ** 2)
            collision_risk = self.calculate_collision_risk(new_x, new_y, new_yaw)

            # Update the trajectory cost with a combination of distance to target and collision risk
            trajectory_cost += distance_to_target + collision_risk

        return trajectory_cost

    def calculate_collision_risk(self, x, y, yaw):
        # Calculate the collision risk for the given pose (x, y, yaw) using laser scan data

        # Define some collision risk parameters
        collision_threshold = 0.3
        half_scan_range = len(self.scan_ranges) // 2

        # Get the index of the scan range closest to the robot's forward direction
        robot_heading_index = half_scan_range

        # Calculate the collision risk by checking scan ranges in the robot's heading direction
        collision_risk = 0.0
        for i in range(half_scan_range - 45, half_scan_range + 45):
            scan_range = self.scan_ranges[i]

            # Convert the scan index to the corresponding angle
            angle = self.scan_angle_min + self.scan_angle_increment * i

            # Calculate the position of the obstacle in the scan range
            obstacle_x = x + scan_range * math.cos(yaw + angle)
            obstacle_y = y + scan_range * math.sin(yaw + angle)

            # Calculate the distance to the obstacle
            distance_to_obstacle = math.sqrt((obstacle_x - x) ** 2 + (obstacle_y - y) ** 2)

            # Increase collision risk if obstacle is close
            if distance_to_obstacle < collision_threshold:
                collision_risk += 1.0

        return collision_risk

    def generate_linear_velocities(self):
        # Generate a range of linear velocities
        num_steps = 5
        step_size = self.max_linear_velocity / num_steps
        velocities = [i * step_size for i in range(num_steps + 1)]
        return velocities

    def generate_angular_velocities(self):
        # Generate a range of angular velocities
        num_steps = 10
        step_size = self.max_angular_velocity / num_steps
        velocities = [-self.max_angular_velocity + i * step_size for i in range(num_steps + 1)]
        return velocities

    def shutdown(self):
        rospy.loginfo("Stopping the TurtleBot...")
        self.cmd_vel.publish(Twist())  # Stop the TurtleBot
        rospy.sleep(1)
        self.conn.close()  # Close the database connection

if __name__ == '__main__':
    try:
        turtlebot = TurtleBot()
        target_coordinates = []
        while True:
            target_input = input("Enter target coordinates (x,y) or 'done' to finish: ")
            if target_input.lower() == 'done':
                break
            else:
                x, y = target_input.split(",")
                target_coordinates.append((float(x), float(y)))
        turtlebot.move_to_target(target_coordinates)
    except rospy.ROSInterruptException:
        pass
```

### Upload Odometry to MySQL

```python
    def insert_coordinates(self, x, y, theta):
        try:
            # Connect to the database
            conn = pymysql.connect(host='192.168.1.155', user='turtlebot', password='0000', database='test')
            cursor = conn.cursor()

            query = "UPDATE one SET x = %s, y = %s, theta = %s WHERE idone = 1"
            self.cursor.execute(query, (x, y, theta))
            self.conn.commit()

        except pymysql.Error as e:
            print(f"Error in update_position: {e}")
    def move_to_target(self, target_coordinates):
        self.target_coordinates = target_coordinates

        for i in range(len(self.target_coordinates)):
            target_x, target_y = self.target_coordinates[i]
            rospy.loginfo("Moving to target: ({}, {})".format(target_x, target_y))

            while not rospy.is_shutdown():
                # Get the current target coordinates
                target_x, target_y = self.target_coordinates[i]

                # Calculate the distance to the current target
                current_x = self.current_pose.position.x
                current_y = self.current_pose.position.y
                distance_to_target = math.sqrt((target_x - current_x) ** 2 + (target_y - current_y) ** 2)

                # Calculate the angle to the target
                target_angle = math.atan2(target_y - current_y, target_x - current_x)

                # Calculate the angular velocity to align with the target angle
                current_orientation = (
                    self.current_pose.orientation.x,
                    self.current_pose.orientation.y,
                    self.current_pose.orientation.z,
                    self.current_pose.orientation.w
                )
                _, _, current_yaw = euler_from_quaternion(current_orientation)
                angle_difference = target_angle - current_yaw

                # Insert current coordinates into the database
                self.insert_coordinates(current_x, current_y, current_yaw)
```

- pymysqlì„ ì‚¬ìš©í•˜ì—¬ í…Œì´ë¸” oneì— ì‹¤ì‹œê°„ x, y, theta ì…ë ¥

### Stop sign detection and stopping

```python
    def check_stop(self):
        try:
            # Retrieve the collision value from the three table
            query = "SELECT stop FROM three order by idthree desc limit 1"
            self.cursor.execute(query)
            result = self.cursor.fetchone()

            if result is None:
                return 0
            else:
                return result[0]
                
    def move_to_target(self, target_coordinates):

                # Check for stop signal from the database
                if self.check_stop() == 1:
                    twist_cmd = Twist()
                    twist_cmd.angular.z = 0.0
                    twist_cmd.linear.x = 0.0
                    self.cmd_vel.publish(twist_cmd)
                    rospy.loginfo("Stopped by external signal.")
                    return
```

- í…Œì´ë¸” threeì˜ stop ë°ì´í„°ë¥¼ ì§€ì†ì ìœ¼ë¡œ ì½ê³  ë°ì´í„°ê°€ 1ì´ ë˜ë©´ ì¶©ëŒì„ ê°ì§€í•˜ê³  ê¸´ê¸‰ ì •ì§€

# Unreal Engine 4

![ue4_overall](https://github.com/JeongMin-D/GraduationProject/assets/38097923/91f316f8-19c1-408e-a878-283c5af98b98)

- Level blueprint

## Import AMR data from MySQL

![plugin](https://github.com/JeongMin-D/GraduationProject/assets/38097923/343bbf5e-3553-4697-bf19-844183757338)

- MySQLê³¼ ì—°ë™ê°€ëŠ¥í•œ í”ŒëŸ¬ê·¸ì¸ ì‚¬ìš©

![sql_open](https://github.com/JeongMin-D/GraduationProject/assets/38097923/a4447c37-4aa3-41c1-8475-50209b4f3615)

- MySQLDBConnection ì•¡í„° ë¸”ë£¨í”„ë¦°íŠ¸ ìƒì„± í›„ ì‚¬ìš©í•  MySQL Databaseì™€ ì—°ê²° ì„¤ì •

![sql_data_input_output](https://github.com/JeongMin-D/GraduationProject/assets/38097923/6404f99d-bae4-4c55-8726-475b18ba5a29)

- Tick ì´ë²¤íŠ¸ë¡œ ì‹¤ì‹œê°„ two, one í…Œì´ë¸” ë°ì´í„° Select 

![sql_data_input](https://github.com/JeongMin-D/GraduationProject/assets/38097923/29d933cd-c01c-4e2a-b637-deb3f08adaa2)

- ì‹¤ì‹œê°„ìœ¼ë¡œ Selectí•œ ë°ì´í„°ë¥¼ Rowë³„ë¡œ ì˜ë¼ì„œ ObstacleData ì´ë²¤íŠ¸ ë””ìŠ¤íŒ¨ì²˜ë¡œ ì „ì†¡
- ë ˆë²¨ ë¸”ë£¨í”„ë¦°íŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•¨

![ue4_turtlebot](https://github.com/JeongMin-D/GraduationProject/assets/38097923/0f885660-a6ae-4f2e-b334-f0e65f42f213)

- ë ˆë²¨ ë¸”ë£¨í”„ë¦°íŠ¸ì—ì„œ ObstacleDataë¥¼ í• ë‹¹í•˜ê³  x, y, thetaë¥¼ ìœ„ì¹˜ë¡œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì„œ Turtlebot ì•¡í„°ì— ë°ì´í„° ì „ì†¡í•˜ì—¬ ìœ„ì¹˜ ì´ë™

## Receive object information from YOLOv5 and spawn object

![sql_open](https://github.com/JeongMin-D/GraduationProject/assets/38097923/5bab6233-47f1-4c20-8a92-976b2f230d49)

- ìœ„ ì‚¬ì§„ê³¼ ë˜‘ê°™ì€ MySQLDBConnection ì•¡í„° ë¸”ë£¨í”„ë¦°íŠ¸ ì‚¬ìš©

![sql_data_input_output](https://github.com/JeongMin-D/GraduationProject/assets/38097923/f3ad0c01-228b-47e6-856e-ac82a0646002)

- Sequence, ë™ì¼í•œ MySQLDBConnection ì•¡í„° ìƒì„±ì„ í•´ë³´ì•˜ì§€ë§Œ ì˜¤ë¥˜ë¡œ ì¸í•´ update ë¶ˆê°€
- í•œë²ˆì— 2ê°œì˜ ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•´ì•¼í•˜ëŠ” ê²ƒì´ ë¬¸ì œì¸ ê²ƒìœ¼ë¡œ ìƒê°í•˜ë‚˜ ê²€ì¦ ë¶ˆê°€
- ë”°ë¼ì„œ Aì™€ Bë¥¼ í•œë²ˆì”© ë²ˆê°ˆì•„ê°€ë©° ì‹¤í–‰í•˜ëŠ” Flip Flopìœ¼ë¡œ í•´ê²°

![ue4_object](https://github.com/JeongMin-D/GraduationProject/assets/38097923/27fa9d2a-04b7-4618-bbc7-e8d2c150b1d6)

- ê°ì§€ëœ ê°ì²´ê°€ 0ì´ ì•„ë‹Œ ê²½ìš° ì¤‘ ìŠ¤í°ë˜ì§€ ì•Šì€ ê°ì²´ëŠ” ìŠ¤í°, ìŠ¤í°ëœ ê²½ìš°ëŠ” ì´ë™ë˜ë„ë¡ ì½”ë“œ êµ¬ì„±
- 5ì´ˆê°„ ê°ì²´ê°€ ê°ì§€ë˜ì§€ ì•Šì•„ì„œ ë°ì´í„°ë² ì´ìŠ¤ê°€ 0ì´ ë˜ë©´ ìŠ¤í°ëœ ê°ì²´ëŠ” ì‚­ì œ
- ê°ì²´ì˜ x, y ìœ„ì¹˜ì—ì„œ ë°ì´í„°ë¥¼ ë½‘ì•„ ìŠ¤í° ë° ìœ„ì¹˜ ì´ë™ì— ì‚¬ìš©

## Detect conflicting signals and upload to MySQL

![overlap_pic](https://github.com/JeongMin-D/GraduationProject/assets/38097923/9d5ea4a7-228d-4aa6-acf6-7ec6e2e2e620)

- ì¥ì• ë¬¼ ê°ì§€ë¥¼ ìœ„í•´ Turtlebot ì•¡í„°ì— Box Collision ìƒì„±

![overlap](https://github.com/JeongMin-D/GraduationProject/assets/38097923/956a3841-1fa7-438b-8d2a-cbd40c5c4a2a)

- Box Collisionì— ë‹¤ë¥¸ ë¬¼ì²´ê°€ ê²¹ì³ì§€ëŠ” ìˆœê°„ ì¥ì• ë¬¼ë¡œ ê°ì§€í•˜ê³  Sendsignalì— True ê°’ì„ ì‹¤ì–´ì„œ ë ˆë²¨ ë¸”ë£¨í”„ë¦°íŠ¸ë¡œ ì „ì†¡

![ue4_collision_signal](https://github.com/JeongMin-D/GraduationProject/assets/38097923/5c92e0b4-85d6-4586-8891-9c784daf36aa)

- ë ˆë²¨ ë¸”ë£¨í”„ë¦°íŠ¸ì—ì„œ Sendsignalì„ í• ë‹¹í•˜ê³  True ë°ì´í„°ë¥¼ ë°›ì€ ê²½ìš° ë³€ìˆ˜ì— stop, Trueë¥¼ ë°›ì§€ ëª»í•œ ê²½ìš° moveë¡œ ì €ì¥

![sql_data_input_output](https://github.com/JeongMin-D/GraduationProject/assets/38097923/58c9d7f5-8885-4465-975a-1ede4c6d5fcc)

- ìœ„ì™€ ê°™ì€ MySQLDBConnectionì—ì„œ ë³€ìˆ˜ê°€ stopì¼ ê²½ìš° ë°ì´í„°ë² ì´ìŠ¤ three í…Œì´ë¸” stop ê°’ì„ 1, moveì¼ ê²½ìš° 0ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ì—¬ ì¶©ëŒ ì‹ í˜¸ ì „ì†¡

# Result

![1](https://github.com/JeongMin-D/GraduationProject/assets/38097923/5d4d268c-0b57-4b67-8282-a5992de7a0ba)

![4](https://github.com/JeongMin-D/GraduationProject/assets/38097923/ab02c612-30da-4f9b-a1d9-d1c21fc50bff)

- 0.6 ì´ìƒì˜ boxë¥¼ YOLOv5ë¡œ ê°ì§€í•˜ê³ , ê·¸ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ two í…Œì´ë¸”ì— ì „ì†¡í•˜ì—¬ ì—…ë¡œë“œ
- Turtlebotì˜ Odometryì˜ x, y, theta ê°’ì„ ë°ì´í„°ë² ì´ìŠ¤ one í…Œì´ë¸”ì— ì „ì†¡í•˜ì—¬ ì—…ë¡œë“œ
- Unreal Engine 4ì—ì„œ oneê³¼ two í…Œì´ë¸”ì˜ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ ê°€ìƒí™˜ê²½ì—ì„œ êµ¬í˜„
- ê°€ìƒí™˜ê²½ì— êµ¬í˜„ëœ boxì™€ Turtlebotì´ ì´ë™ ì¤‘ ì¶©ëŒí•˜ê²Œ ë˜ë©´ three í…Œì´ë¸”ì— ì¶©ëŒ ì‹ í˜¸ ì „ì†¡
- Turtlebot ì½”ë“œì—ì„œ three í…Œì´ë¸”ì˜ ì¶©ëŒì‹ í˜¸ë¥¼ selectí•˜ë‹¤ê°€ stopì˜ ê°’ì´ 1ë¡œ ì—…ë°ì´íŠ¸ ë˜ë©´ Turtlebotì— ì •ì§€ì‹ í˜¸ë¥¼ ë³´ë‚´ ì •ì§€
- Turtlebotì´ ì •ì§€í•˜ë©´ Odometry ì •ë³´ê°€ ì—…ë°ì´íŠ¸ ë˜ì§€ ì•Šì•„ ê°€ìƒí™˜ê²½ ë‚´ì—ì„œ ê°™ì´ ì •ì§€
- ë”°ë¼ì„œ, Sensorless Turtlebotì´ ê°ì²´ íƒì§€ ë° ìœ„ì¹˜ ì •ë³´ ê¸°ë°˜ìœ¼ë¡œ ê°€ìƒí™˜ê²½ì„ ë§Œë“¤ì–´ ì¥ì• ë¬¼ì„ ê°ì§€í•˜ê³  ì •ì§€í•˜ëŠ” ê¸°ìˆ  ì œì‘